{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup Python Libraries","metadata":{}},{"cell_type":"code","source":"%%bash\npip install numpy torch datasets transformers evaluate --quiet\npip freeze | grep -E '^numpy|^torch|^datasets|^transformers|^evaluate'","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:56:23.280992Z","iopub.execute_input":"2023-06-23T08:56:23.281339Z","iopub.status.idle":"2023-06-23T08:56:37.150006Z","shell.execute_reply.started":"2023-06-23T08:56:23.281308Z","shell.execute_reply":"2023-06-23T08:56:37.149171Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","output_type":"stream"},{"name":"stdout","text":"datasets==2.1.0\nevaluate==0.4.0\nnumpy==1.23.5\ntorch @ file:///tmp/torch/torch-2.0.0-cp310-cp310-linux_x86_64.whl\ntorchaudio @ file:///tmp/torch/torchaudio-2.0.1-cp310-cp310-linux_x86_64.whl\ntorchdata==0.6.0\ntorchinfo==1.7.2\ntorchmetrics==0.11.4\ntorchtext @ file:///tmp/torch/torchtext-0.15.1-cp310-cp310-linux_x86_64.whl\ntorchvision @ file:///tmp/torch/torchvision-0.15.1-cp310-cp310-linux_x86_64.whl\ntransformers==4.28.1\n","output_type":"stream"},{"name":"stderr","text":"\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create SMS Spam Dataset for Training BERT","metadata":{"execution":{"iopub.status.busy":"2023-05-22T03:48:19.14472Z","iopub.execute_input":"2023-05-22T03:48:19.146039Z","iopub.status.idle":"2023-05-22T03:48:19.1513Z","shell.execute_reply.started":"2023-05-22T03:48:19.14598Z","shell.execute_reply":"2023-05-22T03:48:19.14995Z"}}},{"cell_type":"markdown","source":"## Let's load the SMS Spam Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# we load the sms spam dataset from the huggingface hub [source: https://huggingface.co/datasets/sms_spam]\n\nraw_dataset = load_dataset('sms_spam')\nraw_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:56:43.699236Z","iopub.execute_input":"2023-06-23T08:56:43.699592Z","iopub.status.idle":"2023-06-23T08:56:48.325065Z","shell.execute_reply.started":"2023-06-23T08:56:43.699564Z","shell.execute_reply":"2023-06-23T08:56:48.317552Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547b7e821313468cb04b2ca58de76e27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/901 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d060dcba22e141b1ba345b6e5c92a0b0"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset sms_spam/plain_text (download: 198.65 KiB, generated: 509.53 KiB, post-processed: Unknown size, total: 708.17 KiB) to /root/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84249bce9b64d5092df6731cde2b0d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5574 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset sms_spam downloaded and prepared to /root/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c50e80f55ce44e5ae04c44c6441d680"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 5574\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# let's look at 5 examples of the sms spam data\n# sms: The sms text message\n# label: 1=SPAM and 0=HAM/NOT SPAM\nfor i in range(5):\n    print(f\"SMS[{i}]  : {raw_dataset['train']['sms'][i]}\", end='')\n    print(f\"LABEL[{i}]: {'SPAM' if raw_dataset['train']['label'][i] else 'HAM'}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:56:52.378082Z","iopub.execute_input":"2023-06-23T08:56:52.378723Z","iopub.status.idle":"2023-06-23T08:56:52.445677Z","shell.execute_reply.started":"2023-06-23T08:56:52.378686Z","shell.execute_reply":"2023-06-23T08:56:52.444625Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"SMS[0]  : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\nLABEL[0]: HAM\n\nSMS[1]  : Ok lar... Joking wif u oni...\nLABEL[1]: HAM\n\nSMS[2]  : Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\nLABEL[2]: SPAM\n\nSMS[3]  : U dun say so early hor... U c already then say...\nLABEL[3]: HAM\n\nSMS[4]  : Nah I don't think he goes to usf, he lives around here though\nLABEL[4]: HAM\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Let's create a train, validation, and test set ","metadata":{}},{"cell_type":"code","source":"# we first split train to 80% of total and test to 20% of total\ntrain_test_dataset = raw_dataset['train'].train_test_split(test_size=0.2, seed=42, shuffle=True)\ntrain_test_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:56:57.397577Z","iopub.execute_input":"2023-06-23T08:56:57.398127Z","iopub.status.idle":"2023-06-23T08:56:57.428671Z","shell.execute_reply.started":"2023-06-23T08:56:57.398087Z","shell.execute_reply":"2023-06-23T08:56:57.427813Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 4459\n    })\n    test: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 1115\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# we then split the validation test to 10% of total and test set to 10% of total\nval_test_dataset = train_test_dataset['test'].train_test_split(test_size=0.5, seed=42, shuffle=True)\nval_test_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:05.536468Z","iopub.execute_input":"2023-06-23T08:57:05.536837Z","iopub.status.idle":"2023-06-23T08:57:05.554279Z","shell.execute_reply.started":"2023-06-23T08:57:05.536806Z","shell.execute_reply":"2023-06-23T08:57:05.552834Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 557\n    })\n    test: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 558\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# we get a 80% train, 10% validation, 10% test\ndataset = DatasetDict({\n    'train': train_test_dataset['train'],\n    'val': val_test_dataset['train'],\n    'test': val_test_dataset['test'],\n})\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:08.273582Z","iopub.execute_input":"2023-06-23T08:57:08.273973Z","iopub.status.idle":"2023-06-23T08:57:08.281776Z","shell.execute_reply.started":"2023-06-23T08:57:08.273942Z","shell.execute_reply":"2023-06-23T08:57:08.280686Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 4459\n    })\n    val: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 557\n    })\n    test: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 558\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Setup Training (Fine-Tuning) on BERT","metadata":{}},{"cell_type":"markdown","source":"## We start by tokenizing our dataset","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:16.992228Z","iopub.execute_input":"2023-06-23T08:57:16.992699Z","iopub.status.idle":"2023-06-23T08:57:21.283142Z","shell.execute_reply.started":"2023-06-23T08:57:16.992656Z","shell.execute_reply":"2023-06-23T08:57:21.281962Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016e93e7a422476a97af4c1ccf533db2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec49022a15140009e7d2f0f6c9101b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f616ef27da849db9a892e006ea60f1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5953d471a5b1499a82acb85b407a542d"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"},"metadata":{}}]},{"cell_type":"code","source":"def tokenization(example):\n    # we lower case the examples as the model is uncased \n    lowercased_example = list(map(str.lower, example['sms']))\n    # we only truncate due to data collator's dynamic padding\n    return tokenizer(lowercased_example, truncation=True)\n\n# we tokenize the sms in batches and remove the original column\ntokenized_dataset = dataset.map(tokenization, batched=True, remove_columns=['sms'])\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:23.847824Z","iopub.execute_input":"2023-06-23T08:57:23.848680Z","iopub.status.idle":"2023-06-23T08:57:24.558660Z","shell.execute_reply.started":"2023-06-23T08:57:23.848639Z","shell.execute_reply":"2023-06-23T08:57:24.557663Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b22d5d9ad8415698f3cccbf63a7dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f7849c4f7b1487caeb0795f6744f047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"212476e897aa4863bba00b1988fc3523"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 4459\n    })\n    val: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 557\n    })\n    test: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 558\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## We setup a data collator for dynamic padding","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\n# we setup a data collator for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:29.110457Z","iopub.execute_input":"2023-06-23T08:57:29.110837Z","iopub.status.idle":"2023-06-23T08:57:37.389458Z","shell.execute_reply.started":"2023-06-23T08:57:29.110803Z","shell.execute_reply":"2023-06-23T08:57:37.388517Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## We setup the training metrics (Accuracy, F1)","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\naccuracy_metric = evaluate.load('accuracy')\nf1_metric = evaluate.load('f1')\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    f1 = f1_metric.compute(predictions=predictions, references=labels)\n    return {**accuracy, **f1}","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:49.153673Z","iopub.execute_input":"2023-06-23T08:57:49.154530Z","iopub.status.idle":"2023-06-23T08:57:52.251398Z","shell.execute_reply.started":"2023-06-23T08:57:49.154491Z","shell.execute_reply":"2023-06-23T08:57:52.250483Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1609d1164094c9aba7c1fef66b6dd86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f09c4aa97ae4d12a8f8d9ceede2927c"}},"metadata":{}}]},{"cell_type":"markdown","source":"## We disable WandB Logging","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:57:54.734210Z","iopub.execute_input":"2023-06-23T08:57:54.734578Z","iopub.status.idle":"2023-06-23T08:57:54.744890Z","shell.execute_reply.started":"2023-06-23T08:57:54.734547Z","shell.execute_reply":"2023-06-23T08:57:54.743738Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## We setup the BERT model as a binary classifier","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ncheckpoint = 'distilbert-base-uncased'\nnum_labels = 2\nid2label = {0:'HAM',1:'SPAM'}\nlabel2id = {'HAM':0,'SPAM':1}\n\n# source: https://huggingface.co/distilbert-base-uncased\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:58:02.225292Z","iopub.execute_input":"2023-06-23T08:58:02.225668Z","iopub.status.idle":"2023-06-23T08:58:04.744342Z","shell.execute_reply.started":"2023-06-23T08:58:02.225629Z","shell.execute_reply":"2023-06-23T08:58:04.743444Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b80c6bd8d74a2f95b634f11d2958ab"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## We setup Training Configurations ","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# training configuration\n# 1. seed: for reproducibility\n# 2. output_dir: for outputing the model\n# 3. num_train_epochs: we train 3 epochs\n# 4. save_strategy: we save on every epoch to output_dir\n# 5. evaluation_strategy: we evaluate on validation set on every epoch\n# 6. load_best_model_at_end: we load the best model with the lowest validation loss\ntraining_args = TrainingArguments(\n    seed=42,\n    output_dir='./results',\n    num_train_epochs=3,\n    save_strategy='epoch',\n    evaluation_strategy='epoch',\n    load_best_model_at_end=True,\n)\n\n# we setup the trainer with all our previous configurations\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['val'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:58:19.996869Z","iopub.execute_input":"2023-06-23T08:58:19.997257Z","iopub.status.idle":"2023-06-23T08:58:24.607220Z","shell.execute_reply.started":"2023-06-23T08:58:19.997227Z","shell.execute_reply":"2023-06-23T08:58:24.606291Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Let's see how UnFine-Tuned BERT performs as a Baseline","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(tokenized_dataset['test'])","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:58:30.277850Z","iopub.execute_input":"2023-06-23T08:58:30.278215Z","iopub.status.idle":"2023-06-23T08:58:32.387286Z","shell.execute_reply.started":"2023-06-23T08:58:30.278187Z","shell.execute_reply":"2023-06-23T08:58:32.386368Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 00:58]\n    </div>\n    "},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.7021474242210388,\n 'eval_accuracy': 0.3064516129032258,\n 'eval_f1': 0.19206680584551147,\n 'eval_runtime': 2.0981,\n 'eval_samples_per_second': 265.95,\n 'eval_steps_per_second': 33.363}"},"metadata":{}}]},{"cell_type":"markdown","source":"Without fine-tuning BERT, our model currently has around **30% Accuracy (eval_accuracy)** and **19% F1 (eval_f1)**, which is quite bad. \n\nLet's make it better with transfer learning! ","metadata":{}},{"cell_type":"markdown","source":"# Train (Fine-Tune) BERT","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-23T08:59:08.571127Z","iopub.execute_input":"2023-06-23T08:59:08.571487Z","iopub.status.idle":"2023-06-23T09:00:19.893875Z","shell.execute_reply.started":"2023-06-23T08:59:08.571456Z","shell.execute_reply":"2023-06-23T09:00:19.892986Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1674' max='1674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1674/1674 01:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.084200</td>\n      <td>0.062177</td>\n      <td>0.987433</td>\n      <td>0.954248</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.021700</td>\n      <td>0.058818</td>\n      <td>0.991023</td>\n      <td>0.966887</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.012600</td>\n      <td>0.053558</td>\n      <td>0.992819</td>\n      <td>0.973684</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1674, training_loss=0.03539377264901016, metrics={'train_runtime': 71.3009, 'train_samples_per_second': 187.613, 'train_steps_per_second': 23.478, 'total_flos': 188258941324320.0, 'train_loss': 0.03539377264901016, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Let's see how a Fine-Tuned BERT performs","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(tokenized_dataset['test'])","metadata":{"execution":{"iopub.status.busy":"2023-06-23T09:00:53.799465Z","iopub.execute_input":"2023-06-23T09:00:53.800358Z","iopub.status.idle":"2023-06-23T09:00:54.527431Z","shell.execute_reply.started":"2023-06-23T09:00:53.800310Z","shell.execute_reply":"2023-06-23T09:00:54.526469Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 00:00]\n    </div>\n    "},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.07984426617622375,\n 'eval_accuracy': 0.989247311827957,\n 'eval_f1': 0.9565217391304348,\n 'eval_runtime': 0.7161,\n 'eval_samples_per_second': 779.236,\n 'eval_steps_per_second': 97.754,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Let's try out some examples!","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# we setup the spam classifier as a text classification with our custom model and pretrained tokenizer\nspam_classifier = pipeline(task='text-classification', model=model, tokenizer=tokenizer, device=torch.cuda.current_device())","metadata":{"execution":{"iopub.status.busy":"2023-06-23T09:01:09.047430Z","iopub.execute_input":"2023-06-23T09:01:09.047812Z","iopub.status.idle":"2023-06-23T09:01:09.350841Z","shell.execute_reply.started":"2023-06-23T09:01:09.047780Z","shell.execute_reply":"2023-06-23T09:01:09.349883Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Let's try out some real-world spams!\n# This is a spam message I got personally, please do not click it.\n\nsms = \"\"\"\n+26.787$ burn out in 24 hours, Let it have drowned, http://bit.ly/7ayp\n\"\"\"\n\nspam_classifier(sms)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T09:01:10.941958Z","iopub.execute_input":"2023-06-23T09:01:10.942932Z","iopub.status.idle":"2023-06-23T09:01:10.965800Z","shell.execute_reply.started":"2023-06-23T09:01:10.942884Z","shell.execute_reply":"2023-06-23T09:01:10.964688Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[{'label': 'SPAM', 'score': 0.9988980293273926}]"},"metadata":{}}]},{"cell_type":"code","source":"# Let's try out some real-word hams!\n# This is a ham message I got personally.\n\nsms = \"\"\"\nHey want to cook something together tonight?\n\"\"\"\nspam_classifier(sms)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T09:01:14.516554Z","iopub.execute_input":"2023-06-23T09:01:14.517049Z","iopub.status.idle":"2023-06-23T09:01:14.535015Z","shell.execute_reply.started":"2023-06-23T09:01:14.517009Z","shell.execute_reply":"2023-06-23T09:01:14.532789Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'label': 'HAM', 'score': 0.9999334812164307}]"},"metadata":{}}]}]}